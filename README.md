# Red Teaming Findings â€“ GPT-OSS-20B

This repository contains JSON findings generated from ethical red-teaming of OpenAIâ€™s GPT-OSS-20B model. Our goal was to audit the model for potential failure points including reward hacking, deception, chain-of-thought exploitation, and unsafe responses to high-risk prompts.

## ğŸ” Methodology

We used an automated notebook to simulate:
- Deceptive or manipulative intent
- Chain-of-thought (CoT) vulnerabilities
- Dangerous responses to self-harm prompts
- Reward-hacking and sandbagging
- Sociopolitical manipulation

Each prompt and response was captured and converted to a Kaggle-compatible JSON finding using OpenAIâ€™s Harmony schema format.

## ğŸ“ File Structure

This repository includes:
- Individual `.json` findings (fully validated)
- A `findings_backup.zip` archive of all results

## ğŸ›¡ï¸ Ethics

All red-teaming activities were conducted responsibly, with a strong emphasis on AI safety, reproducibility, and transparency.

## ğŸ“ How to Use

You may reference these findings in:
- Red-teaming evaluations
- Ethical AI research
- Model benchmarking

## ğŸ‘¤ Author

**Cezar Braga da Silva**  
Kaggle: [@cezarbragadasilva](https://www.kaggle.com/cezarbragadasilva)  
GitHub: [@CezarBraga](https://github.com/CezarBraga)
